# 一些卷积核

## 空洞卷积(dilated Convolution)

### 为什么引入空洞卷积？
引入空洞卷积不得不提的是感受野，感受野就是卷积神经网络的每一层输出的特征图(feature map)上的像素点在原图像上映射的区域大小。空洞卷积主要为了解决图像分割中的一些问题而提出的，在FCN中通过pooling增大感受野缩小图像尺寸，然后通过upsampling还原图像尺寸，但是这个过程中造成了精度的损失，那么为了减小这种损失理所当然想到的是去掉pooling层，然而这样就导致特征图感受野太小，因此空洞卷积应运而生。

### 什么是空洞卷积？
空洞卷积的两种理解(stride=1,padding:0)：
* 将卷积核扩展：如卷积核为3*3但将其变为5*5的卷积核并在每行每列中加0.
* 在特征图上每隔一行或一列取数与3*3卷积核进行卷积。
* ![dilated Convolution)](https://github.com/ly-gump/CNN/tree/main/figures/0conv.gif)

## 转置卷积(Transpose Convolution)(上采样)

### 上采样
在我们使用神经网络的过程中,我们经常需要上采样(up-sampling)来提高低分辨率图片的分辨率。上采样有诸多方法,如最近邻插值、双线性插值和双立方插值，但是上述的方法都需要插值操作, 而这些插值的操作都充满了人为设计和特征工程的气息, 而且也没有网络进行学习的余地.

### 为什么需要转置卷积？
如果我们想要网络去学出一种最优的上采样方法,我们可以使用转置卷积.它与基于插值的方法不同,它有可以学习的参数。转置卷积也被称作: "分数步长卷积(Fractionally-strided convolution)“和"反卷积(Deconvolution)”。

### 卷积的逆向操作
考虑一下如何与卷积换一个计算方向. 也就是说,我们想要建立在一个矩阵中的1个值和另外一个矩阵中的9(3*3)个值的关系.这就是像在进行卷积的逆向操作,这就是转置卷积的核心思想。

我们可以将卷积操作写成一个矩阵. 其实这就是重新排列一下kernel矩阵, 使得我们通过一次矩阵乘法就能计算出卷积操作后的矩阵.假设我们转置一下卷积矩阵 C 和一个2*2列向量以矩阵乘法相乘,就能得到4*4的output矩阵.

还可以直接用卷积核在图像上滑动来理解：
* ![transpose Convolution)](https://github.com/ly-gump/CNN/tree/main/figures/tconv.gif)

